{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Loading data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Scaling\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "plt.imshow(X_train[:,i].reshape(28,28), cmap = matplotlib.cm.binary)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "print(y_train[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_r = np.reshape(x_train, \n",
    "                       (x_train.shape[0], \n",
    "                        x_train.shape[1] * x_train.shape[2]))\n",
    "test_x_r = np.reshape(x_test, \n",
    "                      (x_test.shape[0], \n",
    "                       x_test.shape[1] * x_test.shape[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Unsupervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "\n",
    "mod = decomposition.PCA(n_components=2)\n",
    "mod.fit(train_x_r)\n",
    "\n",
    "print(mod.explained_variance_ratio_)\n",
    "print(mod.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_r_t = mod.transform(train_x_r)\n",
    "\n",
    "# Scatterplot colored by label\n",
    "plt.scatter(train_x_r_t[:,0], \n",
    "            train_x_r_t[:,1], \n",
    "            c = list(y_train), \n",
    "            cmap=plt.get_cmap('jet', 10), s = 0.01)\n",
    "\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = decomposition.PCA(n_components=3)\n",
    "mod.fit(train_x_r)\n",
    "\n",
    "print(mod.explained_variance_ratio_)\n",
    "print(mod.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "train_x_r_t = mod.transform(train_x_r)\n",
    "\n",
    "# Same scatterplot as above, but with three dimensions.\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "ax.scatter(train_x_r_t[:,0], \n",
    "           train_x_r_t[:,1], \n",
    "           train_x_r_t[:,2], \n",
    "           cmap=plt.get_cmap('jet', 10), \n",
    "           c = list(y_train), s = 0.01)\n",
    "ax.set_xlabel('Component 1')\n",
    "ax.set_ylabel('Component 2')\n",
    "ax.set_zlabel('Component 3')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = decomposition.PCA(n_components=100)\n",
    "mod.fit(train_x_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cumulative variance explained by component\n",
    "plt.plot(mod.explained_variance_ratio_.cumsum())\n",
    "plt.xlabel(\"Components\")\n",
    "plt.ylabel(\"Cumulative Variance Explained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Support vector machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 The kernel trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "X1 = np.random.normal(0,1.5,size = 500)\n",
    "X2 = np.random.normal(0,1.5,size = 500)\n",
    "y = X1 + X2 > 2\n",
    "\n",
    "condition = X1 + X2 < 1\n",
    "\n",
    "X1 = X1[(y + condition)]\n",
    "X2 = X2[(y + condition)]\n",
    "y = y[(y + condition)]\n",
    "\n",
    "X = np.column_stack((X1,X2))\n",
    "\n",
    "clf = svm.SVC(kernel='linear', C=1000)\n",
    "clf.fit(X, y)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=30, \n",
    "            cmap=plt.cm.Paired)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(kernel='linear', C=1000)\n",
    "clf.fit(X, y)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=30, \n",
    "            cmap=plt.cm.Paired)\n",
    "\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "\n",
    "xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "YY, XX = np.meshgrid(yy, xx)\n",
    "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "Z = clf.decision_function(xy).reshape(XX.shape)\n",
    "\n",
    "ax.contour(XX, YY, Z, colors='k', \n",
    "           levels=[-1, 0, 1], alpha=0.5,\n",
    "           linestyles=['--', '-', '--'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = np.random.normal(0,1.5,size = 500)\n",
    "X2 = np.random.normal(0,1.5,size = 500)\n",
    "X = np.column_stack((X1,X2))\n",
    "y = np.round((X1+X2)+np.random.normal(0,1,size = 500))>1.5\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel='linear', C=1000)\n",
    "clf.fit(X, y)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)\n",
    "\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "\n",
    "xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "YY, XX = np.meshgrid(yy, xx)\n",
    "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "Z = clf.decision_function(xy).reshape(XX.shape)\n",
    "\n",
    "ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n",
    "           linestyles=['--', '-', '--'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = np.random.normal(0,3,size = 500)\n",
    "X2 = np.random.normal(0,3,size = 500)\n",
    "\n",
    "y = (X1**2+X2**2) < 8\n",
    "\n",
    "condition = X1**2 + X2**2 > 14\n",
    "\n",
    "X1 = X1[(y + condition)]\n",
    "X2 = X2[(y + condition)]\n",
    "y = y[(y + condition)]\n",
    "\n",
    "X = np.column_stack((X1,X2))\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi(X1,X2):\n",
    "    a = X1**2\n",
    "    b = X2**2\n",
    "    c = np.sqrt(2)*X1*X2\n",
    "    return a,b,c\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "a,b,c = phi(X1,X2)\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "ax.scatter(a,b,c, c = y, cmap=plt.cm.Paired)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 SVMs in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly subset 10000 training examples\n",
    "selector = np.random.randint(train_x_r.shape[0], size=10000)\n",
    "train_x_r_subset = train_x_r[selector, :]\n",
    "y_train_subset = y_train[selector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# Initiate the four models\n",
    "linearmodel = svm.SVC(kernel='linear', gamma = 'auto')\n",
    "polymodel = svm.SVC(kernel='poly', degree=2, gamma = 'auto')\n",
    "sigmoidkernel = svm.SVC(kernel='sigmoid', gamma = 'auto')\n",
    "expmodel = svm.SVC(kernel='rbf', gamma = 'auto')\n",
    "\n",
    "# Train them\n",
    "linearmodel.fit(train_x_r_subset, y_train_subset)\n",
    "polymodel.fit(train_x_r_subset, y_train_subset)\n",
    "sigmoidkernel.fit(train_x_r_subset, y_train_subset)\n",
    "expmodel.fit(train_x_r_subset, y_train_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Get area under the curve for each of the models\n",
    "\n",
    "print(roc_auc_score(pd.get_dummies(y_train_subset), \n",
    "                    pd.get_dummies(linearmodel.predict(train_x_r_subset))))\n",
    "print(roc_auc_score(pd.get_dummies(y_train_subset), \n",
    "                    pd.get_dummies(polymodel.predict(train_x_r_subset))))\n",
    "print(roc_auc_score(pd.get_dummies(y_train_subset), \n",
    "                    pd.get_dummies(sigmoidkernel.predict(train_x_r_subset))))\n",
    "print(roc_auc_score(pd.get_dummies(y_train_subset), \n",
    "                    pd.get_dummies(expmodel.predict(train_x_r_subset))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We take a sample of just 1000 data points \n",
    "\n",
    "selector = np.random.randint(train_x_r.shape[0], size=1000)\n",
    "train_x_r_subset = train_x_r[selector, :]\n",
    "y_train_subset = y_train[selector]\n",
    "\n",
    "# Perform PCA on the subset, with just two components\n",
    "# so we can plot the points in a 2D scatterplot\n",
    "\n",
    "mod = decomposition.PCA(n_components=2)\n",
    "mod.fit(train_x_r_subset)\n",
    "train_x_r_subset_t = mod.transform(train_x_r_subset)\n",
    "\n",
    "# We then fit the models\n",
    "\n",
    "models = (svm.SVC(kernel='linear', gamma = 'auto'),\n",
    "          svm.SVC(kernel='poly', degree=2, gamma = 'auto'),\n",
    "          svm.SVC(kernel='sigmoid', gamma = 'auto'),\n",
    "          svm.SVC(kernel='rbf', gamma = 'auto'))\n",
    "\n",
    "models = (clf.fit(train_x_r_subset_t, y_train_subset) for clf in models)\n",
    "\n",
    "# And lastly plot the results with the decision boundaries\n",
    "\n",
    "def make_meshgrid(x, y, h=.02):\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "\n",
    "def plot_contours(ax, clf, xx, yy, **params):\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    return out\n",
    "\n",
    "titles = ('SVC with linear kernel',\n",
    "          'SVC with polynomial (degree 2) kernel',\n",
    "          'SVC with sigmoid kernel',\n",
    "          'SVC with RBF kernel')\n",
    "\n",
    "fig, sub = plt.subplots(2, 2)\n",
    "plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "X0, X1 = train_x_r_subset_t[:, 0], train_x_r_subset_t[:, 1]\n",
    "xx, yy = make_meshgrid(X0, X1)\n",
    "\n",
    "for clf, title, ax in zip(models, titles, sub.flatten()):\n",
    "    plot_contours(ax, clf, xx, yy,\n",
    "                  cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "    ax.scatter(X0, X1, c=y_train_subset, \n",
    "               cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    ax.set_title(title)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Reload the data taking just 500 data points\n",
    "selector = np.random.randint(train_x_r.shape[0], size=500)\n",
    "train_x_r_subset = train_x_r[selector, :]\n",
    "y_train_subset = y_train[selector]\n",
    "\n",
    "# Define the parameters values we want to test\n",
    "\n",
    "tuned_parameters = [{'kernel': ['rbf'], \n",
    "                     'gamma': [0.0001, 0.0001, 0.001, 0.01, 0.1, 1],\n",
    "                     'C': [0.01, 0.1, 1, 10, 100, 1000, 10000]}]\n",
    "\n",
    "# Perform grid search\n",
    "\n",
    "clf = GridSearchCV(svm.SVC(), tuned_parameters, cv=5,\n",
    "                       scoring='accuracy')\n",
    "clf.fit(train_x_r_subset, y_train_subset)\n",
    "\n",
    "# Print the parameters that maximized accuracy\n",
    "print(clf.best_params_)\n",
    "\n",
    "# We will also print all the results\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select half the dataset to reduce computation time\n",
    "selector = np.random.randint(train_x_r.shape[0], size=30000)\n",
    "train_x_r_subset = train_x_r[selector, :]\n",
    "y_train_subset = y_train[selector]\n",
    "\n",
    "# Train the model\n",
    "expmodel = svm.SVC(kernel='rbf', gamma = 0.01, C = 10)\n",
    "expmodel.fit(train_x_r_subset, y_train_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model's performance\n",
    "\n",
    "print(roc_auc_score(pd.get_dummies(y_test), \n",
    "                    pd.get_dummies(expmodel.predict(test_x_r))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Neural Netwokrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Backpropagation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Neural network from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "# Load dataset\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Reshaping to have 784 features instead of 28x28\n",
    "x_train_r = np.reshape(x_train, (x_train.shape[0], x_train.shape[1] * x_train.shape[2]))\n",
    "x_test_r = np.reshape(x_test, (x_test.shape[0], x_test.shape[1] * x_test.shape[2]))\n",
    "\n",
    "# One hot encoding\n",
    "y_train_r = np.array(pd.get_dummies(y_train))\n",
    "y_test_r = np.array(pd.get_dummies(y_test))\n",
    "\n",
    "# Transpose so each column is a training/testing example\n",
    "X_train = x_train_r.T\n",
    "X_test = x_test_r.T\n",
    "Y_train = y_train_r.T\n",
    "Y_test = y_test_r.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "    return s\n",
    "\n",
    "def multiclass_loss(Y, Y_hat, s):\n",
    "\n",
    "    C = -(1/s) * np.sum(Y * np.log(Y_hat))\n",
    "\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, I, H, J):\n",
    "        self.I = I\n",
    "        self.H = H\n",
    "        self.J = J\n",
    "        \n",
    "        self.W1 = np.random.randn(self.H, self.I)\n",
    "        self.b1 = np.zeros((self.H, 1))\n",
    "        self.W2 = np.random.randn(self.J, self.H)\n",
    "        self.b2 = np.zeros((self.J, 1))\n",
    "\n",
    "    # Feedforward, as described in the previous section\n",
    "    def feedforward(self, X):\n",
    "        z1 = (self.W1 @ X) + self.b1\n",
    "        a1 = sigmoid(z1)\n",
    "        z2 = (self.W2 @ a1) + self.b2\n",
    "        a2 = np.exp(z2) / np.sum(np.exp(z2), axis=0)\n",
    "        self.output = a2\n",
    "        \n",
    "    # Backpropagation, as described in the previous section. The\n",
    "    # only change is that we use minibatchsize.\n",
    "    def backpropagation(self, X, Y, minibatchsize, epochs, eta):\n",
    "        s = minibatchsize\n",
    "        K = X.shape[1]\n",
    "        for i in range(0,epochs):\n",
    "            c = 0\n",
    "            for mb in range(int(K//(s+1))):\n",
    "                Xs = X[:,c:c+s]\n",
    "                Ys = Y[:,c:c+s]\n",
    "                c += s + 1\n",
    "                \n",
    "                z1 = (self.W1 @ Xs) + self.b1\n",
    "                a1 = sigmoid(z1)\n",
    "                z2 = (self.W2 @ a1) + self.b2\n",
    "                a2 = np.exp(z2) / np.sum(np.exp(z2), axis=0)\n",
    "                \n",
    "                cost = multiclass_loss(Ys, a2, s)\n",
    "                a2minusy = (a2-Ys)\n",
    "                dW2 = (1/(s+1)) * (a2minusy @ a1.T)\n",
    "                db2 = (1/(s+1)) * np.sum(a2minusy, axis = 1, keepdims=True)\n",
    "\n",
    "                derivative = self.W2.T @ (a2-Ys) * (sigmoid(z1)*(1-sigmoid(z1)))\n",
    "                dW1 = (1/(s+1)) * (derivative @ Xs.T)\n",
    "                db1 = (1/(s+1)) * np.sum(derivative, axis=1, keepdims=True)\n",
    "\n",
    "                self.W2 = self.W2 - eta * dW2\n",
    "                self.b2 = self.b2 - eta * db2\n",
    "\n",
    "                self.W1 = self.W1 - eta * dW1\n",
    "                self.b1 = self.b1 - eta * db1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(784,100,10)\n",
    "nn.feedforward(X_test)\n",
    "# The predicted and true y are one-hot encoded. Using np.argmax\n",
    "# outputs the predicted or true label\n",
    "print(np.argmax(nn.output, axis = 0))\n",
    "print(np.argmax(Y_test, axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(784,100,10)\n",
    "# Train the network\n",
    "nn.backpropagation(X_train,Y_train, minibatchsize = 50, epochs = 2, eta = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt\n",
    "\n",
    "# Calculate AUC score\n",
    "nn.feedforward(X_test)\n",
    "print(roc_auc_score(Y_test.T, nn.output.T))\n",
    "\n",
    "# Plot ROC curves for each class\n",
    "labels = np.argmax(Y_test, axis = 0)\n",
    "skplt.metrics.plot_roc(labels, nn.output.T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Building a neural network using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.python.keras.optimizers import SGD\n",
    "\n",
    "# We reload the original dataset, rescale and one-hot code.\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "y_train_hot = pd.get_dummies(y_train)\n",
    "y_test_hot = pd.get_dummies(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "# Flatten the input\n",
    "keras.layers.Flatten(input_shape=(28, 28)),\n",
    "# Hidden layer with 1200 units with Relu activation\n",
    "keras.layers.Dense(1200, activation=tf.nn.relu),\n",
    "# Moderately high dropout rate that will avoid overfitting     \n",
    "keras.layers.Dropout(0.4),\n",
    "# Second hidden layer, also with Relu\n",
    "keras.layers.Dense(1200, activation=tf.nn.relu),\n",
    "# Output layer with 10 units and softmax activation\n",
    "keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
    "\n",
    "# We use categorical crossentropy as a loss function as we did \n",
    "# in the network we built from scratch, and additionally we \n",
    "# use Adam optimizer.\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(), \n",
    "          loss='categorical_crossentropy',\n",
    "          metrics=['accuracy'])\n",
    "\n",
    "# Ten epochs with a batch size of 62\n",
    "model.fit(x_train, y_train_hot, epochs=10, batch_size = 62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test, y_test_hot, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the model is more than 98%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 DS Challange: Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is necessary to add one dimension to the data. In general, CNN deal with\n",
    "# images with various color channels and expect a fourth dimension.\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 1)\n",
    "y_train_hot = pd.get_dummies(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Conv2D(6, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(28,28,1)))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(tf.keras.layers.Conv2D(16, (3, 3), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(120, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(84, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(), \n",
    "          loss='categorical_crossentropy',\n",
    "          metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train_hot,\n",
    "          batch_size=128,\n",
    "          epochs=5,\n",
    "          verbose=1,\n",
    "          validation_split = 0.2,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test, y_test_hot, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizelayer(layer, image = 0):\n",
    "    \n",
    "    img_to_visualize = x_train[image]\n",
    "    img_to_visualize = np.expand_dims(img_to_visualize, axis=0)\n",
    "\n",
    "    inputs = [tf.keras.backend.learning_phase()] + model.inputs\n",
    "    _convout1_f = tf.keras.backend.function(inputs, [model.layers[layer].output])\n",
    "\n",
    "    def convout1_f(X):\n",
    "        return _convout1_f([0]+[X])\n",
    "\n",
    "    convolutions = convout1_f(img_to_visualize)\n",
    "    convolutions = np.squeeze(convolutions)\n",
    "    n = convolutions.shape[0]\n",
    "\n",
    "    if convolutions.shape[2] == 6:\n",
    "        a = 6\n",
    "        b = 1\n",
    "    elif convolutions.shape[2] == 16:\n",
    "        a = 8\n",
    "        b = 2\n",
    "\n",
    "    fig, axs = plt.subplots(b,a, figsize=(15, 4))\n",
    "    axs = axs.ravel()\n",
    "    for i in range(convolutions.shape[2]):\n",
    "        axs[i].imshow(convolutions[:,:,i], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizelayer(0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizelayer(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizelayer(2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizelayer(3, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
